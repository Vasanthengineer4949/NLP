{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMBEDDINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings are vectors that represent words. They are used to represent words in a vector space. It is mathematically represented as a matrix of size (d, V), where d is the dimensionality of the embedding and V is the number of words in the vocabulary. The mapping function is given by the equation:\n",
    "\n",
    "> f: X-> Y, which is a function. \n",
    "\n",
    "\n",
    "Where the function is\n",
    "\n",
    "•\t**injective** (which is what we call an **injective function** , each Y has a unique X correspondence, and vice versa)\n",
    "\n",
    "•\t**structure-preserving** ( structure preservation , for example, X1 < X2 in the space to which X belongs, then the same applies to Y1 <Y2 in the space to which Y belongs after mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Popular translation can be considered as word embedding, **which is to map the words in the space to which X belongs to a multi-dimensional vector in Y space , then the multi-dimensional vector is equivalent to embedding in the space to which Y belongs**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Text data needs to be pre-processed into tensor form before it can be input to the neural network.\n",
    "2. The process of dividing text into units is called tokenization, and the unit of division is called tokens.\n",
    "3. Text can be divided into words, characters (abcdefg ...), n-gram and so on.\n",
    "4. Generally use one-hot encoding or word-embedding to process words into numerical tensors.\n",
    "5. One-hot encoding is simple, but without structure, the distance between any two words is √2.\n",
    "6. The word-embedding space has small dimensions, structure in space, similar words are near, and unrelated words are far away.\n",
    "7. The role of the embedding layer can actually be seen as a matrix that maps the points in the high-dimensional space to the low-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WORD EMBEDDINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embedding is a form of word representation **that connects the human understanding of language to that of the machine**. Word embeddings are the distributed representations of text in an ample dimensional space. By looking at different researches in the area of deep learning, word embeddings are essential. **It is the approach of representing words and documents that may be considered as one of the crucial breakthroughs in the field of deep learning on challenging NLP problems.**\n",
    "\n",
    "Word embeddings are a class of techniques **where the individual word, is represented as a real-valued vector in a vector space**. The main idea is to use a densely distributed representation for all the words.\n",
    "Each word is represented by a real-value vector. Each word is mapped to a single vector, and the vector values are learned in a way that resembles a neural network, and hence the technique is often lumped into the field of deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The neural network cannot train the original text data. We need to process the text data into numerical tensors first. This process is also called text vectorization.**\n",
    "\n",
    "There are several strategies for text vectorization:\n",
    "1. Split text into words, each word is converted into a vector\n",
    "2. Split text into characters, each character is converted into a vector\n",
    "3. Extract n-gram of words or characters n-gram to a vector\n",
    "4. One-hot encoding\n",
    "\n",
    "**The unit into which text is decomposed is called token, and the process of decomposing text into token is called tokenization.**\n",
    "\n",
    "To put it simply, we need to input text data into a neural network and let it train. However, neural networks cannot directly process text data. We need to pre-process text data into a format that the neural network can understand, which is the following process:\n",
    "\n",
    "**Text ----> Participle ----> Vectorization**\n",
    "\n",
    "\n",
    "There are two main methods for word vectorization:\n",
    "1. One-hot encoding\n",
    "2. Word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONE HOT ENCODING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why is it called one-hot?** \n",
    "\n",
    "**After each word is one-hot encoded, only one position has an element of 1 and the other positions are all 0.**\n",
    "\n",
    "For example, \n",
    "the sentence **\"the boy is crying\"** (assuming there are only four English words in the world), after one-hot encoding,\n",
    "\n",
    "**the corresponds to (1, 0, 0, 0)**\n",
    "\n",
    "**boy corresponds to (0, 1, 0 ， 0）**\n",
    "\n",
    "**is corresponds to (0,0,1,0)**\n",
    "\n",
    "**crying corresponds to (0,0,0,1)**\n",
    "\n",
    "Each word corresponds to a position in the vector, and this position represents the word.\n",
    "\n",
    "But this way requires a very high dimension, because if all vocabularies have 100,000 words, then each word needs to be represented by a vector of length 100,000.\n",
    "\n",
    "**the corresponding to (1, 0, 0, 0, ..., 0) (length is 100,000)**\n",
    "\n",
    "**boy corresponding to (0, 1, 0, 0, ..., 0)**\n",
    "\n",
    "**is corresponding to (0, 0, 1, 0 , ..., 0)**\n",
    "\n",
    "**crying corresponds to (0,0,0,1, ..., 0) to get high-dimensional sparse tensors.**\n",
    "\n",
    "<figure>\n",
    "<img src= \"./res/04_1_one-hot.jpg\" style=\"width:100%\">\n",
    "<figcaption align = \"center\"><b>Fig.1 -  One Hot Encoding</b></figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADVANTAGES AND DISADVANTAGES OF ONE HOT ENCODING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADVANTAGES\n",
    "\n",
    "* One-hot encoding is a simple way to represent words.\n",
    "* One-hot encoding is a sparse representation.\n",
    "* The working of one-hot encoding is easy to understand.\n",
    "\n",
    "## DISADVANTAGES\n",
    "\n",
    "* The length of the word vector is equal to the length of the vocabulary, and the word vector is extremely sparse. When the vocabulary is large, the computational complexity will be very large.\n",
    "\n",
    "* Any two words are orthogonal, meaning that the relationship between words cannot be obtained from the One-Hot code\n",
    "\n",
    "* The distance between any two words is equal, and the semantic relevance of the two words cannot be reflected from the distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAG OF WORDS - BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bag-of-words model is a commonly used document representation method in the field of information retrieval .**\n",
    "\n",
    "In information retrieval, the BOW model assumes that for a document, it ignores its word order, grammar, syntax and other factors, and treats it as a collection of several words. The appearance of each word in the document is independent and independent of whether other words appear. **(It's out of order)**\n",
    "\n",
    "The Bag-of-words model (BoW model) ignores the grammar and word order of a text, and uses a set of unordered words to express a text or a document.\n",
    "\n",
    "#### Let's take an example\n",
    "\n",
    "`John likes to watch movies. Mary likes too.`\n",
    "\n",
    "`John also likes to watch football games.`\n",
    "\n",
    "Build a dictionary based on the words that appear in the above two sentences:\n",
    "\n",
    "`{\"John\": 1, \"likes\": 2, \"to\": 3, \"watch\": 4, \"movies\": 5, \"also\": 6, \"football\": 7, \"games\": 8, \"Mary\": 9, \"too\": 10}`\n",
    "\n",
    "\n",
    "The dictionary contains 10 words, each word has a unique index. Note that their order is not related to the order in which they appear in the sentence. According to this dictionary, we re-express the above two sentences into the following two vectors:\n",
    "\n",
    "`[1, 2, 1, 1, 1, 0, 0, 0, 1, 1]`\n",
    "\n",
    "`[1, 1, 1, 1, 0, 1, 1, 1, 0, 0]`\n",
    "\n",
    "\n",
    "These two vectors contain a total of 10 elements, where the i-th element represents the number of times the i-th word in the dictionary appears in the sentence. \n",
    "\n",
    "Now imagine a **huge document set D with a total of M documents**. After all the words in the document are extracted, they form a dictionary containing N words. Using the Bag-of-words model, **each document can be represented as an N-dimensional vector**.\n",
    "\n",
    "\n",
    "Therefore, the BoW model can be considered as a statistical histogram. It is used in text retrieval and processing applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPLEMENTING BOW USING SCIKIT-LEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   affordable  aim  company  easy  edtech  ineuron  interact  learn  platform  \\\n",
      "0           0    0        1     0       1        1         1      1         1   \n",
      "1           1    1        0     1       0        0         0      1         0   \n",
      "\n",
      "   provides  providing  sota  students  technologies  way  world  \n",
      "0         1          0     0         1             0    0      1  \n",
      "1         0          1     1         1             1    1      0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\oneneuronnlp\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sentence_1=\"Ineuron is an Edtech company that provides a platform for students to learn and interact with the world around them.\"\n",
    "sentence_2=\"It is in the aim of providing an affordable and easy way for students to learn SOTA technologies.\"\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1,1)\n",
    "                        ,stop_words='english')\n",
    "\n",
    "vector = vectorizer.fit_transform([sentence_1,sentence_2])\n",
    "\n",
    "output=pd.DataFrame(vector.toarray(),columns=vectorizer.get_feature_names())\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF VECTORIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF (Term Frequency-Inverse Document Frequency)**, a commonly used weighting technique for information retrieval and information exploration.\n",
    "\n",
    "TF-IDF is a statistical method used to evaluate the importance of a word to a file set or a file in a corpus. The importance of the word increases in proportion to the number of times it appears in the file, but at the same time decreases inversely with the frequency of its appearance in the corpus.\n",
    "\n",
    "<figure>\n",
    "<img src= \"./res/04_2_tfidf.png\">\n",
    "<figcaption><b>Fig.2 -  Formula for TF-IDF</b></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF(TERM FREQUENCY)\n",
    "\n",
    "* **Term frequency TF (item frequency)**: number of times a given word appears in the text. This number is usually normalized (the numerator is generally smaller than the denominator) to prevent it from favoring long documents, because whether the term is important or not, it is likely to appear more often in long documents than in paragraph documents.\n",
    "\n",
    "> **TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).**\n",
    "\n",
    "Term frequency (TF) indicates how often a term (keyword) appears in the text .\n",
    "\n",
    "This number is usually normalized (usually the word frequency divided by the total number of words in the article) to prevent it from favoring long documents.\n",
    "\n",
    "<figure>\n",
    "<img src= \"./res/04_3_tf.png\">\n",
    "<figcaption><b>Fig.3 -  Formula for TF</b></figcaption>\n",
    "</figure>\n",
    "\n",
    "where  ni, j  is the number of occurrences of the word in the file  dj, and the denominator is the sum of the occurrences of all words in the file dj;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDF(INVERSE DOCUMENT FREQUENCY)\n",
    "\n",
    "* **Inverse document frequency (IDF)**: A measure of the general importance of a word. The main idea is that if there are fewer documents containing the entry t and the larger, it means that the entry has a good ability to distinguish categories. The IDF of a specific word can be calculated by dividing the total number of files by the number of files containing the word, and then taking the log of the obtained quotient.\n",
    "\n",
    ">**IDF(t) = log_e(Total number of documents / Number of documents with term t in it).**\n",
    "\n",
    "<figure>\n",
    "<img src= \"./res/04_4_idf.png\">\n",
    "<figcaption><b>Fig.4 -  Formula for IDF</b></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:\n",
    "\n",
    "Consider a document containing 100 words where in the word cat appears 3 times. \n",
    "\n",
    "The **term frequency (Tf) for cat** is then **(3 / 100) = 0.03**. Now, assume we have 10 million documents and the word cat appears in one thousand of these.\n",
    "\n",
    "Then, the **inverse document frequency (Idf)** is calculated as **log(10,000,000 / 1,000) = 4.** \n",
    "\n",
    "Thus, the **Tf-idf** weight is the product of these quantities: **0.03 * 4 = 0.12.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APPLICATIONS OF TF-IDF\n",
    "\n",
    "1.  **Search engine**\n",
    "2.  **Keyword extraction**\n",
    "3.  **Text similarity**\n",
    "4.  **Text summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PYTHONIC IMPLEMENTATION OF TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('to', 0.0322394037469742), ('stop', 0.0322394037469742), ('worthless', 0.0322394037469742), ('my', 0.028288263356383563), ('dog', 0.028288263356383563), ('him', 0.028288263356383563), ('stupid', 0.028288263356383563), ('has', 0.025549122992281622), ('flea', 0.025549122992281622), ('problems', 0.025549122992281622), ('help', 0.025549122992281622), ('please', 0.025549122992281622), ('maybe', 0.025549122992281622), ('not', 0.025549122992281622), ('take', 0.025549122992281622), ('park', 0.025549122992281622), ('dalmation', 0.025549122992281622), ('is', 0.025549122992281622), ('so', 0.025549122992281622), ('cute', 0.025549122992281622), ('I', 0.025549122992281622), ('love', 0.025549122992281622), ('posting', 0.025549122992281622), ('garbage', 0.025549122992281622), ('mr', 0.025549122992281622), ('licks', 0.025549122992281622), ('ate', 0.025549122992281622), ('steak', 0.025549122992281622), ('how', 0.025549122992281622), ('quit', 0.025549122992281622), ('buying', 0.025549122992281622), ('food', 0.025549122992281622)]\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "import operator\n",
    "\n",
    "def loadDataSet():\n",
    "    dataset = [ ['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],    \n",
    "                ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid'] ]\n",
    "    classVec = [0, 1, 0, 1, 0, 1]  \n",
    "    return dataset, classVec\n",
    "\n",
    "\n",
    "def feature_select(list_words):\n",
    "    \n",
    "    doc_frequency=defaultdict(int)\n",
    "    for word_list in list_words:\n",
    "        for i in word_list:\n",
    "            doc_frequency[i]+=1\n",
    "    \n",
    "    word_tf={}  \n",
    "    for i in doc_frequency:\n",
    "        word_tf[i]=doc_frequency[i]/sum(doc_frequency.values())\n",
    "    \n",
    "    doc_num=len(list_words)\n",
    "    word_idf={} \n",
    "    word_doc=defaultdict(int) \n",
    "    for i in doc_frequency:\n",
    "        for j in list_words:\n",
    "            if i in j:\n",
    "                word_doc[i]+=1\n",
    "    for i in doc_frequency:\n",
    "        word_idf[i]=math.log(doc_num/(word_doc[i]+1))\n",
    "    \n",
    "    word_tf_idf={}\n",
    "    for i in doc_frequency:\n",
    "        word_tf_idf[i]=word_tf[i]*word_idf[i]\n",
    "    \n",
    "    dict_feature_select=sorted(word_tf_idf.items(),key=operator.itemgetter(1),reverse=True)\n",
    "    return dict_feature_select\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    data_list,label_list=loadDataSet() \n",
    "    features=feature_select(data_list) \n",
    "    print(features)\n",
    "    print(len(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPLEMENTING TF-IDF USING NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this', 'is', 'sentence', 'one'], ['this', 'is', 'sentence', 'two'], ['this', 'is', 'sentence', 'three']]\n",
      "<Text: this is sentence one this is sentence two...>\n",
      "0.08333333333333333\n",
      "1.0986122886681098\n",
      "0.0915510240556758\n"
     ]
    }
   ],
   "source": [
    "from nltk.text import TextCollection\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sents=['this is sentence one','this is sentence two','this is sentence three']\n",
    "sents=[word_tokenize(sent) for sent in sents]\n",
    "print(sents)\n",
    "corpus=TextCollection(sents)\n",
    "print(corpus)\n",
    "\n",
    "tf=corpus.tf('one',corpus)\n",
    "print(tf)\n",
    "\n",
    "idf=corpus.idf('one')\n",
    "print(idf)\n",
    "\n",
    "tf_idf=corpus.tf_idf('one',corpus)\n",
    "print(tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPLEMENTATION OF TF-IDF USING SCIKIT-LEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output x_train text vector：\n",
      "[[0.22941573 0.22941573 0.22941573 0.45883147 0.22941573 0.22941573\n",
      "  0.22941573 0.22941573 0.45883147 0.45883147]]\n",
      "Output x_test text vector：\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "x_train = ['The main idea of TF-IDF is that algorithm is an important feature that can be separated from the corpus background']\n",
    "x_test=['Original text marked ',' main idea']\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=10)\n",
    "\n",
    "tf_idf_transformer = TfidfTransformer()\n",
    "\n",
    "tf_idf = tf_idf_transformer.fit_transform(vectorizer.fit_transform(x_train))\n",
    "\n",
    "x_train_weight = tf_idf.toarray()\n",
    "\n",
    "tf_idf = tf_idf_transformer.transform(vectorizer.transform(x_test))\n",
    "x_test_weight = tf_idf.toarray()\n",
    "\n",
    "print('Output x_train text vector：')\n",
    "print(x_train_weight)\n",
    "print('Output x_test text vector：')\n",
    "print(x_test_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-GRAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wikipedia definition**: \n",
    "\n",
    "**In computational linguistics, n-gram refers to n consecutive items in the text (items can be phoneme, syllable, letter, word or base pairs)**\n",
    "\n",
    "N-grams of texts are widely used in the field of text mining and natural language processing. They are basically a set of co-occurring words within a defined window and when computing the n-grams, we typically move one word forward or more depending upon the scenario.\n",
    "\n",
    "For example, for the sentence **“The cow jumps over the moon”**. If **N=2** (known as bigrams), then the ngrams would be:\n",
    "\n",
    "* the cow\n",
    "* cow jumps\n",
    "* jumps over\n",
    "* over the\n",
    "* the moon\n",
    "\n",
    "In n-gram, **n = 1 is unigram**, **n = 2 is bigram**, **n = 3 is trigram**. \n",
    "After **n> 4**, refer directly to numbers, such as **4-gram, 5-gram**.\n",
    "gram is often used to compare sentence similarity, fuzzy query, sentence rationality, sentence correction, etc.\n",
    "\n",
    "<figure>\n",
    "<img src= \"./res/04_5_ngram.png\", style=\"width:100%\">\n",
    "<figcaption align=\"center\"><b>Fig.5 -  N Gram Formula Approximation</b></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determination of N in N-gram\n",
    "\n",
    "To confirm the value of N. \"Language Modeling with Ngrams\" uses the indicator **Perplexity**. The smaller the indicator, the better the effect of a language model. \n",
    "\n",
    ">The article uses a Wall Street Journal database with a dictionary size of 19,979. The training set contains 38 million words and the test set contains 1.5 million words. \n",
    "\n",
    "For different N-grams, calculate their respective perplexity.\n",
    "\n",
    "<figure>\n",
    "<img src= \"./res/04_6_perp_formula.png\", style=\"width:100%\">\n",
    "<figcaption align=\"center\"><b>Fig.6 -  Perplexity Calculation forumula</b></figcaption>\n",
    "</figure>\n",
    "\n",
    "The results show that Tri-gram's Perplexity is the smallest, so it works best.\n",
    "\n",
    "<figure>\n",
    "<img src= \"./res/04_7_perp_result.png\", style=\"width:100%\">\n",
    "<figcaption align=\"center\"><b>Fig.7 -  Perplexity Result</b></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNIGRAM IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'will', 'go', 'to', 'United', 'States']\n"
     ]
    }
   ],
   "source": [
    "sent = \"I will go to United States\"\n",
    "lst_sent = sent.split (\" \")\n",
    "of_unigrams_in = []\n",
    "for i in range(len(lst_sent)):\n",
    "    of_unigrams_in.append(lst_sent[i])\n",
    "print(of_unigrams_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIGRAM IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I will', 'will go', 'go to', 'to United', 'United States']\n"
     ]
    }
   ],
   "source": [
    "sent = \"I will go to United States\"\n",
    "lst_sent = sent.split (\" \")\n",
    "of_bigram_in = []\n",
    "for i in range(len(lst_sent)-1):\n",
    "    of_bigram_in.append(lst_sent[i] + \" \" + lst_sent[i+1])\n",
    "print(of_bigram_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRIGRAM IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I will go', 'will go to', 'go to United', 'to United States']\n"
     ]
    }
   ],
   "source": [
    "sent = \"I will go to United States\"\n",
    "lst_sent = sent.split (\" \")\n",
    "of_trigram_in = []\n",
    "for i in range(len(lst_sent)-2):\n",
    "    of_trigram_in.append(lst_sent[i] + \" \" + lst_sent[i+1] + \" \" + lst_sent[i+2])\n",
    "print(of_trigram_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USE OF NGRAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CountVectorizer class of scikit-learn package implements ngram\n",
    "* This is used as a step before vectorizing using TF_IDF by using the **co-occurence matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GloVe is an unsupervised learning algorithm for obtaining vocabulary vector representations. The aggregated global word co-occurrence statistics from the corpus are trained and the resulting representations show interesting linear substructures of the word vector space.**\n",
    "\n",
    "\n",
    "Official website homepage address: <a href=\"https://nlp.stanford.edu/projects/glove/\" target=\"_blank\">https://nlp.stanford.edu/projects/glove/</a>\n",
    "\n",
    "Github: <a href=\"https://github.com/stanfordnlp/GloVe\" target=\"_blank\">https://github.com/stanfordnlp/GloVe</a>\n",
    "\n",
    "Paper download address: <a href=\"https://nlp.stanford.edu/pubs/glove.pdf\" target=\"_blank\">https://nlp.stanford.edu/pubs/glove.pdf</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe word vector format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe is a type of Word embedding. The format of the GloVe word vector and word2vec is a little different from the Stanford open source code training. **The first line of the model trained by word2vec is: thesaurus size and dimensions, while gloVe does not**\n",
    "\n",
    "Word2vec training format:\n",
    "\n",
    "    Size Dimension\n",
    "\n",
    "    Word1 vector1\n",
    "    Word2 vector1\n",
    "    ....\n",
    "    WordN vectorN\n",
    "    \n",
    "\n",
    "GloVe training format:\n",
    "\n",
    "\n",
    "    Word1 vector1\n",
    "    Word2 vector1\n",
    "    ....\n",
    "    WordN vectorN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Therefore, we use the model trained by Glove to add a line of Vocabulary Size in front, and the model is used in the same way as word2vec. The official website provides a lot of word vector models trained using thesaurus, which can be downloaded and used directly. It can be used with the help of **gensim** package as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORD2VEC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec is a collection of models that are used to generate embeddings for any given sentence of our choice. The models that are present in Word2Vec are very shallow neural networks. It consists of only one input layer, one hidden layer and one output layer.\n",
    "\n",
    "The Word2Vec model utilizes two main types of architectures:\n",
    "* Skip-gram\n",
    "* CBOW\n",
    "\n",
    "<figure>\n",
    "<img src= \"./res/04_8_word2vec.png\" style=\"width:100%\">\n",
    "<figcaption align=\"center\"><b>Fig.8 -  Word2Vec</b></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW(Continuous Bag of Words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CBOW model predicts a given current word in the context of words within a specific window in the text. Here the input layer consists of the context words and the output layer consists of the current word. The hidden layer of this architecture is a fully connected layer that contains the number of dimensions in which we want to represent the current word i.e. the dimension of the word vector(embedding).\n",
    "\n",
    "<figure>\n",
    "<img src= \"./res/04_9_CBOW.png\" style=\"width:100%\">\n",
    "<figcaption align=\"center\"><b>Fig.9 -  CBOW Architecture</b></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKIP-GRAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip gram predicts the surrounding context words for a given current word within a given specific window of text. Here in this architecturem the input layer contains the current word and the output layer contains the context words. The hidden layer contains the number of dimensions in which we want to represent current word present at the input layer (i.e) the dimension of the word vector(embedding).\n",
    "\n",
    "<figure>\n",
    "<img src= \"./res/04_10_skip.png\" style=\"width:100%\">\n",
    "<figcaption align=\"center\"><b>Fig.10 -  Skip Gram Architecture</b></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WORD2VEC IMPLEMENTATION USING GENSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ineuron', 'is', 'an', 'edtech', 'company', 'that', 'provides', 'platform', 'for', 'students', 'to', 'learn', 'and', 'interact', 'with', 'the', 'world', 'around', 'them']\n",
      "[('world', 0.21892882883548737), ('students', 0.21617504954338074), ('and', 0.09320056438446045), ('platform', 0.0928734838962555), ('learn', 0.07977091521024704), ('to', 0.06299277395009995), ('the', 0.05441128462553024), ('for', 0.02752850018441677), ('provides', 0.016657229512929916), ('is', -0.010736537165939808)]\n",
      "-0.111435644\n",
      "[-5.3868821e-04  2.3831481e-04  5.1017073e-03  9.0057505e-03\n",
      " -9.3020368e-03 -7.1141296e-03  6.4635910e-03  8.9763561e-03\n",
      " -5.0250809e-03 -3.7653032e-03  7.3808716e-03 -1.5371587e-03\n",
      " -4.5333384e-03  6.5539577e-03 -4.8574321e-03 -1.8212905e-03\n",
      "  2.8798427e-03  9.9633494e-04 -8.2910145e-03 -9.4572818e-03\n",
      "  7.3053432e-03  5.0678705e-03  6.7687123e-03  7.6206285e-04\n",
      "  6.3510793e-03 -3.4057270e-03 -9.4230933e-04  5.7612793e-03\n",
      " -7.5225709e-03 -3.9330292e-03 -7.5007030e-03 -9.3456963e-04\n",
      "  9.5337275e-03 -7.3197219e-03 -2.3387205e-03 -1.9300305e-03\n",
      "  8.0772610e-03 -5.9305630e-03  4.4327044e-05 -4.7610719e-03\n",
      " -9.5972745e-03  5.0015636e-03 -8.7600593e-03 -4.3927361e-03\n",
      " -3.2266893e-05 -2.9116325e-04 -7.6643988e-03  9.6112955e-03\n",
      "  4.9758586e-03  9.2292484e-03 -8.1558991e-03  4.4845985e-03\n",
      " -4.1425028e-03  8.2568137e-04  8.4865633e-03 -4.4673700e-03\n",
      "  4.5197615e-03 -6.7958804e-03 -3.5556103e-03  9.3876254e-03\n",
      " -1.5746488e-03  3.1687572e-04 -4.1266191e-03 -7.6849405e-03\n",
      " -1.5135751e-03  2.4783162e-03 -8.8958134e-04  5.5356696e-03\n",
      " -2.7536894e-03  2.2561676e-03  5.4565845e-03  8.3549311e-03\n",
      " -1.4486322e-03 -9.1976486e-03  4.3706326e-03  5.7392661e-04\n",
      "  7.4421167e-03 -8.0973632e-04 -2.6429354e-03 -8.7474324e-03\n",
      " -8.4990740e-04  2.8161174e-03  5.3989808e-03  7.0498833e-03\n",
      " -5.7041710e-03  1.8496308e-03  6.0886778e-03 -4.8012058e-03\n",
      " -3.1076819e-03  6.7988113e-03  1.6316612e-03  1.9711052e-04\n",
      "  3.4726427e-03  2.1421221e-04  9.6273664e-03  5.0679650e-03\n",
      " -8.9088297e-03 -7.0481743e-03  9.0591382e-04  6.3866940e-03]\n"
     ]
    }
   ],
   "source": [
    "## Implementing Word2Vec using gensim\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts\n",
    "\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "sentence = \"Ineuron is an Edtech company that provides a platform for students to learn and interact with the world around them.\"\n",
    "sentence = gensim.utils.simple_preprocess(sentence)\n",
    "print(sentence)\n",
    "w2vvectorizer = Word2Vec([sentence], window=5, min_count=1, workers=4)\n",
    "w2vvectorizer.build_vocab([sentence])\n",
    "w2vvectorizer.train([sentence], total_examples=w2vvectorizer.corpus_count, epochs=w2vvectorizer.epochs)\n",
    "print(w2vvectorizer.wv.most_similar('ineuron'))\n",
    "print(w2vvectorizer.wv.similarity('ineuron', 'edtech'))\n",
    "print(w2vvectorizer.wv.get_vector('ineuron'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOC2VEC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc2vec (also known as: paragraph2vec or sentence embedding) is the modified version of word2vec. The main objective of doc2vec is to convert sentence or paragraph to vector (numeric) form. In Natural Language Processing Doc2Vec is used to find related sentences for a given sentence (instead of word in Word2Vec).\n",
    "\n",
    "The doc2vec models may be used in the following way: for training, a set of documents is required. A word vector W is generated for each word, and a document vector D is generated for each document. The model also trains weights for a softmax hidden layer. In the inference stage, a new document may be presented, and all weights are fixed to calculate the document vector.\n",
    "\n",
    "Here it has two main architectures:\n",
    "* PV-DM\n",
    "* PV-DBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PV-DM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PV-DM is similar to CBOW, it's because it's a little addition to the CBOW model. Instead of merely utilising words to anticipate the following word, we additionally included a document-unique feature vector.\n",
    "\n",
    "\n",
    "As a result, when the word vectors W are trained, the document vector D is also trained, and it carries a numeric representation of the document at the end of the training.\n",
    "\n",
    "\n",
    "The model described above is known as the Distributed Memory version of the Paragraph Vector model (PV-DM). It serves as a recollection, recalling what is absent from the current context — or as the paragraph's theme. The document vector seeks to represent the concept of a document, whereas the word vectors convey the concept of a word in a document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PV-DBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another technique, similar to skip-gram, could be utilised in the same way as word2vec,  Paragraph Vector with a Distributed Bag of Words version (PV-DBOW) because there is no need to preserve the word vectors, this approach is actually faster (than word2vec) and uses less memory.\n",
    "\n",
    "\n",
    "The authors of the paper propose combining the two algorithms, even though the PV-DM model is superior and can generally reach state-of-the-art results on its own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DOC2VEC IMPLEMENTATION USING GENSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TaggedDocument(words=['ineuron', 'is', 'an', 'edtech', 'company', 'that', 'provides', 'a', 'platform', 'for', 'students', 'to', 'learn', 'and', 'interact', 'with', 'the', 'world', 'around', 'them', '.'], tags=[0]), TaggedDocument(words=['it', 'aims', 'to', 'provide', 'the', 'platform', 'in', 'an', 'affordable', 'way', '.'], tags=[1]), TaggedDocument(words=['it', 'encourages', 'students', 'to', 'learn', 'the', 'trending', 'technologies'], tags=[2])]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "doc = [\"Ineuron is an Edtech company that provides a platform for students to learn and interact with the world around them.\", \n",
    "\"It aims to provide the platform in an affordable way.\",\n",
    "\"It encourages students to learn the trending technologies\"]\n",
    "\n",
    "tokenized_doc = []\n",
    "for d in doc:\n",
    "    tokenized_doc.append(word_tokenize(d.lower()))\n",
    "tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(tokenized_doc)]\n",
    "print(tagged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('that', 0.5715950727462769), ('around', 0.5659588575363159), ('the', 0.5330694913864136), ('it', 0.4953001141548157), ('learn', 0.49458616971969604), ('in', 0.4902716875076294), ('and', 0.4898565113544464), ('them', 0.44566160440444946), ('encourages', 0.40576207637786865), ('to', 0.3959006667137146)]\n",
      "0.37990546\n"
     ]
    }
   ],
   "source": [
    "d2vvectorizer = Doc2Vec(tagged_data, vector_size=20, window=2, min_count=1, workers=4, epochs = 100)\n",
    "d2vvectorizer.build_vocab(tagged_data)\n",
    "d2vvectorizer.train(tagged_data, total_examples=d2vvectorizer.corpus_count, epochs=d2vvectorizer.epochs)\n",
    "print(d2vvectorizer.wv.most_similar('ineuron'))\n",
    "print(d2vvectorizer.wv.similarity('ineuron', 'edtech'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FASTTEXT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FastText is a Word2Vec enhancement proposed by Facebook in 2016. FastText divides words into many n-grams instead of providing individual words to the Neural Network (sub-words). The trigrams for the word apple, for example, are app, ppl, and ple (ignoring the starting and ending of boundaries of words). The total of these n-grams will be the word embedding vector for apple. We will get word embeddings for all n-grams given the training dataset after training the Neural Network. Rare words can now be adequately represented because some of their n-grams are likely to exist in other words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FASTTEXT IMPLEMENTATION USING GENSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('r', 0.09310827404260635), ('h', 0.07129018753767014), ('e', -0.0678272694349289), ('o', -0.07562373578548431), ('n', -0.0787012130022049), (' ', -0.08875028043985367), ('t', -0.11307767033576965), ('d', -0.13960185647010803), ('a', -0.17304684221744537)]\n",
      "-0.09437718\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "ftsentence  = [\"Ineuron is an Edtech company that provides a platform for students to learn and interact with the world around them.\"]\n",
    "ftvectorizer = FastText(ftsentence, window=5, min_count=5, workers=4,sg=1)\n",
    "ftvectorizer.build_vocab(ftsentence)\n",
    "ftvectorizer.train(ftsentence, total_examples=ftvectorizer.corpus_count, epochs=ftvectorizer.epochs)\n",
    "print(ftvectorizer.wv.most_similar('ineuron'))\n",
    "print(ftvectorizer.wv.similarity('ineuron', 'edtech'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMBEDDINGS FROM PRETRAINED TRANSFORMER MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the State of the Art Transformer Models such as BERT, XLNet, RoBERTa, XLM, DistilBERT, Flaubert, Electra, XLM-R, and XLM-T, we can easily get embeddings for any given sentence.   The embeddings can be used for any purpose, such as for text classification, text generation, or for machine translation. \n",
    "\n",
    "A specific variant of Transformer is sentence transformer, which is a model that is trained to generate embeddings for sentences by which the embeddings can be used to find similarities between sentences or any search query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMBEDDINGS FROM PRETRAINED SENTENCE TRANSFORMER MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d9a6eb342d74bacac2ef48fda6de934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/1.66k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39179ead4a9144539f0c7bce8caca12f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/1.61k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset emotion/default (download: 1.97 MiB, generated: 2.07 MiB, post-processed: Unknown size, total: 4.05 MiB) to C:\\Users\\Admin\\.cache\\huggingface\\datasets\\emotion\\default\\0.0.0\\348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af0242a573ba40a6a07aefd01a20f486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.66M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "688a224acb50425c86184aadd3b495f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/204k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1cf29a4f79f4a3ebc1afbd5c384e6db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/207k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "107c13fb04d64c5bb6438c7f1a5ed633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/16000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c9531bf228d4526b8b48ba972a7844d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ab4353e0ac46979a71d8cf070a1c68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset emotion downloaded and prepared to C:\\Users\\Admin\\.cache\\huggingface\\datasets\\emotion\\default\\0.0.0\\348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "data = load_dataset('emotion', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0                            i didnt feel humiliated      0\n",
       "1  i can go from feeling so hopeless to so damned...      0\n",
       "2   im grabbing a minute to post i feel greedy wrong      3\n",
       "3  i am ever feeling nostalgic about the fireplac...      2\n",
       "4                               i am feeling grouchy      3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.set_format(\"pandas\")\n",
    "df = data[:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_embedding_generator(df, model):\n",
    "    tqdm.pandas()\n",
    "    model = SentenceTransformer(model)\n",
    "    df['embeddings'] = df['text'].progress_apply(lambda x: model.encode(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coltoarr(df):\n",
    "    emb_array = []\n",
    "    for i in tqdm(range(len(df))):\n",
    "        emb_array.append(np.array(df[\"embeddings\"][i]))\n",
    "    emb_array = np.array(emb_array)\n",
    "    print(emb_array.shape)\n",
    "    return emb_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd19b67ad984456aedb5ee764c1603a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbe9504f1dba4e188d6f9e0265acb7ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80a39ddada9b4b52a3ce81eb5a93d2dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/10.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f883e70d40194e2e94bb2d10c1c8f973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3774a4db1f1648769964bd03472745c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00cc30ba9c714292a64f639857de5eaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "864c80ad78b344aea2f5eec9c788244d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "173b9d54bd24459f85cb21553530f34b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2637674c7c7147ca8c8788b780876bfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a0fa90f8e1545d7aabc316717de2ecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3c4cbc5844a43adb4dc51b8609f5c4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aefceeb2f56a498a82dc3c966bd08394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75e2a0ed1228459eb3fc005464d299ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9861af305113419dab7f3a4f11b4c867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16000/16000 [07:05<00:00, 37.61it/s]\n",
      "100%|██████████| 16000/16000 [00:00<00:00, 56737.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000, 384)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>374</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.055050936, -0.0076969415, 0.06353022, -0.0...</td>\n",
       "      <td>-0.055051</td>\n",
       "      <td>-0.007697</td>\n",
       "      <td>0.063530</td>\n",
       "      <td>-0.039664</td>\n",
       "      <td>0.116901</td>\n",
       "      <td>-0.123296</td>\n",
       "      <td>0.058080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063319</td>\n",
       "      <td>-0.044138</td>\n",
       "      <td>-0.034640</td>\n",
       "      <td>0.021249</td>\n",
       "      <td>-0.029084</td>\n",
       "      <td>0.084679</td>\n",
       "      <td>0.016152</td>\n",
       "      <td>0.015425</td>\n",
       "      <td>-0.135161</td>\n",
       "      <td>-0.064534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.009238858, -0.052964333, 0.01926254, 0.0340...</td>\n",
       "      <td>0.009239</td>\n",
       "      <td>-0.052964</td>\n",
       "      <td>0.019263</td>\n",
       "      <td>0.034021</td>\n",
       "      <td>0.125203</td>\n",
       "      <td>0.027428</td>\n",
       "      <td>0.077058</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016320</td>\n",
       "      <td>-0.024402</td>\n",
       "      <td>-0.044897</td>\n",
       "      <td>0.132352</td>\n",
       "      <td>-0.082222</td>\n",
       "      <td>0.003469</td>\n",
       "      <td>0.095559</td>\n",
       "      <td>-0.060182</td>\n",
       "      <td>-0.027176</td>\n",
       "      <td>-0.026275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "      <td>[-0.074502915, -0.010641919, -0.0034595819, -0...</td>\n",
       "      <td>-0.074503</td>\n",
       "      <td>-0.010642</td>\n",
       "      <td>-0.003460</td>\n",
       "      <td>-0.073246</td>\n",
       "      <td>-0.018509</td>\n",
       "      <td>-0.026024</td>\n",
       "      <td>0.023560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050347</td>\n",
       "      <td>-0.030673</td>\n",
       "      <td>-0.001017</td>\n",
       "      <td>0.019752</td>\n",
       "      <td>0.078385</td>\n",
       "      <td>-0.010269</td>\n",
       "      <td>0.041514</td>\n",
       "      <td>-0.024779</td>\n",
       "      <td>-0.042020</td>\n",
       "      <td>0.024512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.10859433, 0.09532226, 0.036476813, 0.015178...</td>\n",
       "      <td>0.108594</td>\n",
       "      <td>0.095322</td>\n",
       "      <td>0.036477</td>\n",
       "      <td>0.015178</td>\n",
       "      <td>0.089073</td>\n",
       "      <td>-0.012647</td>\n",
       "      <td>-0.089686</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019334</td>\n",
       "      <td>-0.076964</td>\n",
       "      <td>-0.004122</td>\n",
       "      <td>0.023587</td>\n",
       "      <td>0.056529</td>\n",
       "      <td>0.024166</td>\n",
       "      <td>0.103731</td>\n",
       "      <td>-0.044091</td>\n",
       "      <td>-0.109329</td>\n",
       "      <td>0.034851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>3</td>\n",
       "      <td>[-0.016712196, -0.078770876, 0.032170076, -0.0...</td>\n",
       "      <td>-0.016712</td>\n",
       "      <td>-0.078771</td>\n",
       "      <td>0.032170</td>\n",
       "      <td>-0.053829</td>\n",
       "      <td>0.115593</td>\n",
       "      <td>-0.051190</td>\n",
       "      <td>0.132093</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011990</td>\n",
       "      <td>0.003192</td>\n",
       "      <td>-0.077645</td>\n",
       "      <td>-0.016146</td>\n",
       "      <td>0.007182</td>\n",
       "      <td>0.029738</td>\n",
       "      <td>0.059137</td>\n",
       "      <td>-0.062703</td>\n",
       "      <td>-0.019559</td>\n",
       "      <td>-0.057704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15995</th>\n",
       "      <td>i just had a very brief time in the beanbag an...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.01957004, 0.06608577, 0.0778042, 0.0881720...</td>\n",
       "      <td>-0.019570</td>\n",
       "      <td>0.066086</td>\n",
       "      <td>0.077804</td>\n",
       "      <td>0.088172</td>\n",
       "      <td>0.091618</td>\n",
       "      <td>0.054001</td>\n",
       "      <td>0.057446</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027110</td>\n",
       "      <td>0.018090</td>\n",
       "      <td>0.031772</td>\n",
       "      <td>0.013136</td>\n",
       "      <td>0.034591</td>\n",
       "      <td>0.068123</td>\n",
       "      <td>-0.029136</td>\n",
       "      <td>-0.083160</td>\n",
       "      <td>-0.095693</td>\n",
       "      <td>-0.019246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15996</th>\n",
       "      <td>i am now turning and i feel pathetic that i am...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.062311042, -0.112134434, 0.055730484, 0.021...</td>\n",
       "      <td>0.062311</td>\n",
       "      <td>-0.112134</td>\n",
       "      <td>0.055730</td>\n",
       "      <td>0.021169</td>\n",
       "      <td>0.025935</td>\n",
       "      <td>-0.031744</td>\n",
       "      <td>-0.054156</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060812</td>\n",
       "      <td>0.052070</td>\n",
       "      <td>-0.014596</td>\n",
       "      <td>0.011752</td>\n",
       "      <td>-0.119441</td>\n",
       "      <td>0.051849</td>\n",
       "      <td>0.018489</td>\n",
       "      <td>-0.046088</td>\n",
       "      <td>-0.026123</td>\n",
       "      <td>0.017155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15997</th>\n",
       "      <td>i feel strong and good overall</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.07894578, -0.0861132, 0.028296798, 0.00922...</td>\n",
       "      <td>-0.078946</td>\n",
       "      <td>-0.086113</td>\n",
       "      <td>0.028297</td>\n",
       "      <td>0.009220</td>\n",
       "      <td>-0.014465</td>\n",
       "      <td>-0.012471</td>\n",
       "      <td>0.109142</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027028</td>\n",
       "      <td>0.037536</td>\n",
       "      <td>-0.066947</td>\n",
       "      <td>0.055402</td>\n",
       "      <td>-0.026460</td>\n",
       "      <td>-0.028652</td>\n",
       "      <td>0.054203</td>\n",
       "      <td>-0.070409</td>\n",
       "      <td>-0.009608</td>\n",
       "      <td>-0.012120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15998</th>\n",
       "      <td>i feel like this was such a rude comment and i...</td>\n",
       "      <td>3</td>\n",
       "      <td>[-0.08410305, -0.040798713, 0.03854899, -0.073...</td>\n",
       "      <td>-0.084103</td>\n",
       "      <td>-0.040799</td>\n",
       "      <td>0.038549</td>\n",
       "      <td>-0.073090</td>\n",
       "      <td>0.067912</td>\n",
       "      <td>0.003937</td>\n",
       "      <td>0.060229</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089333</td>\n",
       "      <td>0.017486</td>\n",
       "      <td>0.007523</td>\n",
       "      <td>0.003251</td>\n",
       "      <td>0.030494</td>\n",
       "      <td>0.037879</td>\n",
       "      <td>0.050217</td>\n",
       "      <td>-0.059059</td>\n",
       "      <td>0.027899</td>\n",
       "      <td>0.026300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15999</th>\n",
       "      <td>i know a lot but i feel so stupid because i ca...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.023683418, -0.045974746, 0.0053985654, -0.0...</td>\n",
       "      <td>0.023683</td>\n",
       "      <td>-0.045975</td>\n",
       "      <td>0.005399</td>\n",
       "      <td>-0.023625</td>\n",
       "      <td>-0.016391</td>\n",
       "      <td>0.079828</td>\n",
       "      <td>0.019790</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009681</td>\n",
       "      <td>-0.036557</td>\n",
       "      <td>0.040831</td>\n",
       "      <td>0.099656</td>\n",
       "      <td>0.004444</td>\n",
       "      <td>-0.006736</td>\n",
       "      <td>0.044252</td>\n",
       "      <td>0.086350</td>\n",
       "      <td>-0.018940</td>\n",
       "      <td>0.006762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16000 rows × 387 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label  \\\n",
       "0                                i didnt feel humiliated      0   \n",
       "1      i can go from feeling so hopeless to so damned...      0   \n",
       "2       im grabbing a minute to post i feel greedy wrong      3   \n",
       "3      i am ever feeling nostalgic about the fireplac...      2   \n",
       "4                                   i am feeling grouchy      3   \n",
       "...                                                  ...    ...   \n",
       "15995  i just had a very brief time in the beanbag an...      0   \n",
       "15996  i am now turning and i feel pathetic that i am...      0   \n",
       "15997                     i feel strong and good overall      1   \n",
       "15998  i feel like this was such a rude comment and i...      3   \n",
       "15999  i know a lot but i feel so stupid because i ca...      0   \n",
       "\n",
       "                                              embeddings         0         1  \\\n",
       "0      [-0.055050936, -0.0076969415, 0.06353022, -0.0... -0.055051 -0.007697   \n",
       "1      [0.009238858, -0.052964333, 0.01926254, 0.0340...  0.009239 -0.052964   \n",
       "2      [-0.074502915, -0.010641919, -0.0034595819, -0... -0.074503 -0.010642   \n",
       "3      [0.10859433, 0.09532226, 0.036476813, 0.015178...  0.108594  0.095322   \n",
       "4      [-0.016712196, -0.078770876, 0.032170076, -0.0... -0.016712 -0.078771   \n",
       "...                                                  ...       ...       ...   \n",
       "15995  [-0.01957004, 0.06608577, 0.0778042, 0.0881720... -0.019570  0.066086   \n",
       "15996  [0.062311042, -0.112134434, 0.055730484, 0.021...  0.062311 -0.112134   \n",
       "15997  [-0.07894578, -0.0861132, 0.028296798, 0.00922... -0.078946 -0.086113   \n",
       "15998  [-0.08410305, -0.040798713, 0.03854899, -0.073... -0.084103 -0.040799   \n",
       "15999  [0.023683418, -0.045974746, 0.0053985654, -0.0...  0.023683 -0.045975   \n",
       "\n",
       "              2         3         4         5         6  ...       374  \\\n",
       "0      0.063530 -0.039664  0.116901 -0.123296  0.058080  ...  0.063319   \n",
       "1      0.019263  0.034021  0.125203  0.027428  0.077058  ... -0.016320   \n",
       "2     -0.003460 -0.073246 -0.018509 -0.026024  0.023560  ...  0.050347   \n",
       "3      0.036477  0.015178  0.089073 -0.012647 -0.089686  ...  0.019334   \n",
       "4      0.032170 -0.053829  0.115593 -0.051190  0.132093  ... -0.011990   \n",
       "...         ...       ...       ...       ...       ...  ...       ...   \n",
       "15995  0.077804  0.088172  0.091618  0.054001  0.057446  ... -0.027110   \n",
       "15996  0.055730  0.021169  0.025935 -0.031744 -0.054156  ... -0.060812   \n",
       "15997  0.028297  0.009220 -0.014465 -0.012471  0.109142  ... -0.027028   \n",
       "15998  0.038549 -0.073090  0.067912  0.003937  0.060229  ...  0.089333   \n",
       "15999  0.005399 -0.023625 -0.016391  0.079828  0.019790  ...  0.009681   \n",
       "\n",
       "            375       376       377       378       379       380       381  \\\n",
       "0     -0.044138 -0.034640  0.021249 -0.029084  0.084679  0.016152  0.015425   \n",
       "1     -0.024402 -0.044897  0.132352 -0.082222  0.003469  0.095559 -0.060182   \n",
       "2     -0.030673 -0.001017  0.019752  0.078385 -0.010269  0.041514 -0.024779   \n",
       "3     -0.076964 -0.004122  0.023587  0.056529  0.024166  0.103731 -0.044091   \n",
       "4      0.003192 -0.077645 -0.016146  0.007182  0.029738  0.059137 -0.062703   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "15995  0.018090  0.031772  0.013136  0.034591  0.068123 -0.029136 -0.083160   \n",
       "15996  0.052070 -0.014596  0.011752 -0.119441  0.051849  0.018489 -0.046088   \n",
       "15997  0.037536 -0.066947  0.055402 -0.026460 -0.028652  0.054203 -0.070409   \n",
       "15998  0.017486  0.007523  0.003251  0.030494  0.037879  0.050217 -0.059059   \n",
       "15999 -0.036557  0.040831  0.099656  0.004444 -0.006736  0.044252  0.086350   \n",
       "\n",
       "            382       383  \n",
       "0     -0.135161 -0.064534  \n",
       "1     -0.027176 -0.026275  \n",
       "2     -0.042020  0.024512  \n",
       "3     -0.109329  0.034851  \n",
       "4     -0.019559 -0.057704  \n",
       "...         ...       ...  \n",
       "15995 -0.095693 -0.019246  \n",
       "15996 -0.026123  0.017155  \n",
       "15997 -0.009608 -0.012120  \n",
       "15998  0.027899  0.026300  \n",
       "15999 -0.018940  0.006762  \n",
       "\n",
       "[16000 rows x 387 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_trans_model = \"sentence-transformers/all-MiniLM-L6-v2\" \n",
    "col_embeddings = transformer_embedding_generator(df, sent_trans_model)\n",
    "embeddings = coltoarr(col_embeddings)\n",
    "embed = pd.DataFrame(embeddings)\n",
    "df = pd.concat([df, embed], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0a4c405f7b19c67e69fc537e9a27167c07b8f40ee97c772a3e7b168555347c6c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('oneneuronnlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
